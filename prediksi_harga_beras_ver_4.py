# -*- coding: utf-8 -*-
"""Prediksi Harga Beras Ver 4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xkfYs6V70sgIZlH0fwdcT2jPSWoADbnr
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.optimizers import RMSprop
from tensorflow.keras.callbacks import EarlyStopping
from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error

# Load dataset
df = pd.read_csv("Dataset Beras Harian Wonosobo.csv", parse_dates=["Tanggal"])
df.set_index("Tanggal", inplace=True)

# Gunakan hanya kolom harga beras premium untuk prediksi
data = df[["Premium"]].values

# Normalisasi data
scaler = MinMaxScaler(feature_range=(0,1))
data_scaled = scaler.fit_transform(data)

# Fungsi untuk membentuk dataset time series
def create_dataset(dataset, time_step=1):
    dataX, dataY = [], []
    for i in range(len(dataset)-time_step-1):
        a = dataset[i:(i+time_step), 0]
        dataX.append(a)
        dataY.append(dataset[i + time_step, 0])
    return np.array(dataX), np.array(dataY)

# Gunakan time step 14
time_step = 14
train_size = int(len(data_scaled) * 0.8)
test_size = len(data_scaled) - train_size
train_data, test_data = data_scaled[:train_size], data_scaled[train_size:]

x_train, y_train = create_dataset(train_data, time_step)
x_test, y_test = create_dataset(test_data, time_step)

# Ubah bentuk data agar sesuai dengan input LSTM
x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)
x_test = x_test.reshape(x_test.shape[0], x_test.shape[1], 1)

# Membangun model LSTM dengan tuning lebih optimal
model = Sequential([
    LSTM(128, activation='relu', return_sequences=True, input_shape=(time_step,1)),
    Dropout(0.1),
    LSTM(128, activation='relu', return_sequences=True),
    Dropout(0.1),
    LSTM(64, activation='relu'),
    Dense(1)
])

# Optimizer dengan learning rate lebih rendah
optimizer = RMSprop(learning_rate=0.0005)
model.compile(loss='mean_squared_error', optimizer=optimizer)

# Callback Early Stopping
early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)

# Training model dengan batch size 32
history = model.fit(x_train, y_train, epochs=200, batch_size=32, validation_data=(x_test, y_test), callbacks=[early_stopping])

# Plot loss training & validation
plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Training & Validation Loss')
plt.legend()
plt.grid()
plt.show()

# Prediksi data
train_predict = model.predict(x_train)
test_predict = model.predict(x_test)

# Inverse transform hasil prediksi
train_predict = scaler.inverse_transform(train_predict.reshape(-1, 1))
y_train_actual = scaler.inverse_transform(y_train.reshape(-1, 1))
test_predict = scaler.inverse_transform(test_predict.reshape(-1, 1))
y_test_actual = scaler.inverse_transform(y_test.reshape(-1, 1))

# Evaluasi model
mse = mean_squared_error(y_test_actual, test_predict)
mape = mean_absolute_percentage_error(y_test_actual, test_predict)
rmse = np.sqrt(mse)
print(f'MSE: {mse}')
print(f'RMSE: {rmse}')
print(f'MAPE: {mape * 100:.2f}%')